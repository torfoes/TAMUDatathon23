{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('quick_draw_data/aircraft carrier.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116504\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(data))\n",
    "# Load the first image from the loaded data\n",
    "image = data[2].reshape(28, 28)\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')  # Turn off the axis numbers\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.892\n",
      "[1, 200] loss: 1.185\n",
      "[1, 300] loss: 0.807\n",
      "[1, 400] loss: 0.654\n",
      "[1, 500] loss: 0.540\n",
      "[1, 600] loss: 0.510\n",
      "[1, 700] loss: 0.456\n",
      "[1, 800] loss: 0.435\n",
      "[1, 900] loss: 0.428\n",
      "[2, 100] loss: 0.389\n",
      "[2, 200] loss: 0.389\n",
      "[2, 300] loss: 0.374\n",
      "[2, 400] loss: 0.392\n",
      "[2, 500] loss: 0.364\n",
      "[2, 600] loss: 0.373\n",
      "[2, 700] loss: 0.343\n",
      "[2, 800] loss: 0.350\n",
      "[2, 900] loss: 0.343\n",
      "[3, 100] loss: 0.347\n",
      "[3, 200] loss: 0.320\n",
      "[3, 300] loss: 0.322\n",
      "[3, 400] loss: 0.332\n",
      "[3, 500] loss: 0.333\n",
      "[3, 600] loss: 0.322\n",
      "[3, 700] loss: 0.318\n",
      "[3, 800] loss: 0.298\n",
      "[3, 900] loss: 0.309\n",
      "[4, 100] loss: 0.314\n",
      "[4, 200] loss: 0.295\n",
      "[4, 300] loss: 0.295\n",
      "[4, 400] loss: 0.294\n",
      "[4, 500] loss: 0.298\n",
      "[4, 600] loss: 0.292\n",
      "[4, 700] loss: 0.285\n",
      "[4, 800] loss: 0.293\n",
      "[4, 900] loss: 0.302\n",
      "[5, 100] loss: 0.283\n",
      "[5, 200] loss: 0.277\n",
      "[5, 300] loss: 0.286\n",
      "[5, 400] loss: 0.301\n",
      "[5, 500] loss: 0.286\n",
      "[5, 600] loss: 0.272\n",
      "[5, 700] loss: 0.259\n",
      "[5, 800] loss: 0.276\n",
      "[5, 900] loss: 0.271\n",
      "Finished Training\n",
      "Accuracy on the test set: 92.62%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()  # Use super() correctly\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 0.077\n",
      "[1, 200] loss: 0.000\n",
      "[1, 300] loss: 0.000\n",
      "[1, 400] loss: 0.000\n",
      "[1, 500] loss: 0.000\n",
      "[1, 600] loss: 0.000\n",
      "[1, 700] loss: 0.000\n",
      "[1, 800] loss: 0.000\n",
      "[1, 900] loss: 0.000\n",
      "[1, 1000] loss: 0.000\n",
      "[1, 1100] loss: 0.000\n",
      "[1, 1200] loss: 0.000\n",
      "[1, 1300] loss: 0.000\n",
      "[1, 1400] loss: 0.000\n",
      "[2, 100] loss: 0.000\n",
      "[2, 200] loss: 0.000\n",
      "[2, 300] loss: 0.000\n",
      "[2, 400] loss: 0.000\n",
      "[2, 500] loss: 0.000\n",
      "[2, 600] loss: 0.000\n",
      "[2, 700] loss: 0.000\n",
      "[2, 800] loss: 0.000\n",
      "[2, 900] loss: 0.000\n",
      "[2, 1000] loss: 0.000\n",
      "[2, 1100] loss: 0.000\n",
      "[2, 1200] loss: 0.000\n",
      "[2, 1300] loss: 0.000\n",
      "[2, 1400] loss: 0.000\n",
      "[3, 100] loss: 0.000\n",
      "[3, 200] loss: 0.000\n",
      "[3, 300] loss: 0.000\n",
      "[3, 400] loss: 0.000\n",
      "[3, 500] loss: 0.000\n",
      "[3, 600] loss: 0.000\n",
      "[3, 700] loss: 0.000\n",
      "[3, 800] loss: 0.000\n",
      "[3, 900] loss: 0.000\n",
      "[3, 1000] loss: 0.000\n",
      "[3, 1100] loss: 0.000\n",
      "[3, 1200] loss: 0.000\n",
      "[3, 1300] loss: 0.000\n",
      "[3, 1400] loss: 0.000\n",
      "[4, 100] loss: 0.000\n",
      "[4, 200] loss: 0.000\n",
      "[4, 300] loss: 0.000\n",
      "[4, 400] loss: 0.000\n",
      "[4, 500] loss: 0.000\n",
      "[4, 600] loss: 0.000\n",
      "[4, 700] loss: 0.000\n",
      "[4, 800] loss: 0.000\n",
      "[4, 900] loss: 0.000\n",
      "[4, 1000] loss: 0.000\n",
      "[4, 1100] loss: 0.000\n",
      "[4, 1200] loss: 0.000\n",
      "[4, 1300] loss: 0.000\n",
      "[4, 1400] loss: 0.000\n",
      "[5, 100] loss: 0.000\n",
      "[5, 200] loss: 0.000\n",
      "[5, 300] loss: 0.000\n",
      "[5, 400] loss: 0.000\n",
      "[5, 500] loss: 0.000\n",
      "[5, 600] loss: 0.000\n",
      "[5, 700] loss: 0.000\n",
      "[5, 800] loss: 0.000\n",
      "[5, 900] loss: 0.000\n",
      "[5, 1000] loss: 0.000\n",
      "[5, 1100] loss: 0.000\n",
      "[5, 1200] loss: 0.000\n",
      "[5, 1300] loss: 0.000\n",
      "[5, 1400] loss: 0.000\n",
      "Finished Training\n",
      "Accuracy on the dataset 1: 100.00%\n",
      "Accuracy on the dataset 2: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load data\n",
    "data0 = np.load('quick_draw_data/basket.npy')\n",
    "data1 = np.load('quick_draw_data/arm.npy')\n",
    "data2 = np.load('quick_draw_data/angel.npy')\n",
    "\n",
    "def prepare_data(data):\n",
    "    images = data[:, :-1].astype(float) / 255.0   # Normalize data to [0, 1]\n",
    "    labels = data[:, -1].astype(int)\n",
    "    \n",
    "    if images.shape[1] == 783:\n",
    "        zeros_column = np.zeros((images.shape[0], 1))\n",
    "        images = np.hstack((images, zeros_column))\n",
    "\n",
    "    assert images.shape[1] == 28*28, f\"Expected 784 pixels but got {images.shape[1]} pixels.\"\n",
    "    \n",
    "    images = torch.tensor(images, dtype=torch.float32).reshape(-1, 1, 28, 28)  # Adding a channel dimension\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    return TensorDataset(images, labels)\n",
    "\n",
    "datasets = [prepare_data(d) for d in [data0, data1, data2]]\n",
    "\n",
    "train_datasets = [random_split(ds, [int(0.8 * len(ds)), len(ds) - int(0.8 * len(ds))])[0] for ds in datasets]\n",
    "test_datasets = [random_split(ds, [int(0.8 * len(ds)), len(ds) - int(0.8 * len(ds))])[1] for ds in datasets]\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(1152, 128) # Instead of nn.Linear(100352, 128)s\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(-1, 128 * 3 * 3)  # Fixed this line\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Using Adam and L2 regularization\n",
    "\n",
    "# Data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "])\n",
    "\n",
    "# Train the model using data0\n",
    "train_loader = DataLoader(train_datasets[0], batch_size=64, shuffle=True)\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Test the model on data1 and data2\n",
    "for idx, test_dataset in enumerate(test_datasets[1:], 1):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy on the dataset {idx}: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1152, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Finished Training\n",
      "Accuracy on the dataset 1: 100.00%\n",
      "Accuracy on the dataset 2: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ... (your existing code)\n",
    "\n",
    "# Print a summary of the model\n",
    "print(\"Model Summary:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# Test the model on data1 and data2\n",
    "for idx, test_dataset in enumerate(test_datasets[1:], 1):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy on the dataset {idx}: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
